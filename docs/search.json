[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan’s Site",
    "section": "",
    "text": "Hello my name is Ethan. I am a Baruch Grad student studying for my masters in statistics.\n\n\n\n\n\n\n\nLast Updated: Tuesday 09 23, 2025 at 20:58PM"
  },
  {
    "objectID": "Mini-Project #01.html",
    "href": "Mini-Project #01.html",
    "title": "Mini-Project #01",
    "section": "",
    "text": "if (!dir.exists(file.path(\"data\", \"mp01\"))) {\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings = FALSE, recursive = TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif (!file.exists(GLOBAL_TOP_10_FILENAME)) {\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile = GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif (!file.exists(COUNTRY_TOP_10_FILENAME)) {\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile = COUNTRY_TOP_10_FILENAME)\n}\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(readr)\nlibrary(dplyr)\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\nstr(GLOBAL_TOP_10)\n\nspc_tbl_ [8,880 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ week                      : Date[1:8880], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:8880] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8880] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8880] \"KPop Demon Hunters\" \"Ruth & Boaz\" \"The Wrong Paris\" \"Man on Fire\" ...\n $ season_title              : chr [1:8880] \"N/A\" \"N/A\" \"N/A\" \"N/A\" ...\n $ weekly_hours_viewed       : num [1:8880] 32200000 15900000 13500000 15700000 11200000 8400000 6800000 6200000 4900000 8400000 ...\n $ runtime                   : num [1:8880] 1.67 1.55 1.78 2.43 1.83 ...\n $ weekly_views              : num [1:8880] 19300000 10300000 7600000 6500000 6100000 4900000 3600000 3200000 3200000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8880] 15 1 3 5 2 1 1 1 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   weekly_hours_viewed = col_double(),\n  ..   runtime = col_double(),\n  ..   weekly_views = col_double(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(GLOBAL_TOP_10)\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …\n\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt;\n    mutate(season_title = if_else(season_title == \"N/A\", NA, season_title))\n\nglimpse(GLOBAL_TOP_10)\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"aka Ch…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …\nlibrary(DT)\nGLOBAL_TOP_10 |&gt; \n    head(n = 20) |&gt;\n    datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n = 20) |&gt;\n    datatable(options = list(searching = FALSE, info = FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n = 20) |&gt;\n    datatable(options = list(searching = FALSE, info = FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(readr)\nlibrary(dplyr)\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME)\n\nstr(COUNTRY_TOP_10)\n\nspc_tbl_ [413,620 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country_name              : chr [1:413620] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n $ country_iso2              : chr [1:413620] \"AR\" \"AR\" \"AR\" \"AR\" ...\n $ week                      : Date[1:413620], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:413620] \"Films\" \"Films\" \"Films\" \"Films\" ...\n $ weekly_rank               : num [1:413620] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:413620] \"Sonic the Hedgehog 3\" \"KPop Demon Hunters\" \"French Lover\" \"She Said Maybe\" ...\n $ season_title              : chr [1:413620] \"N/A\" \"N/A\" \"N/A\" \"N/A\" ...\n $ cumulative_weeks_in_top_10: num [1:413620] 2 15 1 2 1 1 2 5 1 2 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country_name = col_character(),\n  ..   country_iso2 = col_character(),\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(COUNTRY_TOP_10)\n\nRows: 413,620\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"Sonic the Hedgehog 3\", \"KPop Demon Hunters…\n$ season_title               &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 15, 1, 2, 1, 1, 2, 5, 1, 2, 2, 1, 1, 1, …\n\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 |&gt;\n    mutate(season_title = if_else(season_title == \"N/A\", NA, season_title))\n\nglimpse(COUNTRY_TOP_10)\n\nRows: 413,620\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"Sonic the Hedgehog 3\", \"KPop Demon Hunters…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bi…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 15, 1, 2, 1, 1, 2, 5, 1, 2, 2, 1, 1, 1, …\nlibrary(DT)\nCOUNTRY_TOP_10 |&gt; \n    head(n = 20) |&gt;\n    datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\nlibrary(dplyr)\n\ndf &lt;- COUNTRY_TOP_10\n\n\nnum_countries &lt;- n_distinct(df$country_name)"
  },
  {
    "objectID": "Mini-Project #01.html#how-many-different-countries-does-netflix-operate-inyou-can-use-the-viewing-history-as-a-proxy.",
    "href": "Mini-Project #01.html#how-many-different-countries-does-netflix-operate-inyou-can-use-the-viewing-history-as-a-proxy.",
    "title": "Mini-Project #01",
    "section": "How many different countries does Netflix operate in?(You can use the viewing history as a proxy.)",
    "text": "How many different countries does Netflix operate in?(You can use the viewing history as a proxy.)\nThere are 94 that Netflix operates in."
  },
  {
    "objectID": "Mini-Project #01.html#which-non-english-language-film-has-spent-the-most-cumulative-weeks-in-the-global-top-10-how-many-weeks-did-it-spend",
    "href": "Mini-Project #01.html#which-non-english-language-film-has-spent-the-most-cumulative-weeks-in-the-global-top-10-how-many-weeks-did-it-spend",
    "title": "Mini-Project #01",
    "section": "Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?",
    "text": "Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?"
  },
  {
    "objectID": "Mini-Project #01.html#what-is-the-longest-film-english-or-non-english-to-have-ever-appeared-in-the-netflix-global-top-10-how-long-is-it-in-minutes",
    "href": "Mini-Project #01.html#what-is-the-longest-film-english-or-non-english-to-have-ever-appeared-in-the-netflix-global-top-10-how-long-is-it-in-minutes",
    "title": "Mini-Project #01",
    "section": "What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?",
    "text": "What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\nNote that Netflix does not provide runtime for programs before a certain date, so your answer here may be a bit limited."
  },
  {
    "objectID": "Mini-Project #01.html#for-each-of-the-four-categories-what-program-has-the-most-total-hours-of-global-viewership",
    "href": "Mini-Project #01.html#for-each-of-the-four-categories-what-program-has-the-most-total-hours-of-global-viewership",
    "title": "Mini-Project #01",
    "section": "For each of the four categories, what program has the most total hours of global viewership?",
    "text": "For each of the four categories, what program has the most total hours of global viewership?"
  },
  {
    "objectID": "Mini-Project #01.html#which-tv-show-had-the-longest-run-in-a-countrys-top-10-how-long-was-this-run-and-in-what-country-did-it-occur",
    "href": "Mini-Project #01.html#which-tv-show-had-the-longest-run-in-a-countrys-top-10-how-long-was-this-run-and-in-what-country-did-it-occur",
    "title": "Mini-Project #01",
    "section": "Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?",
    "text": "Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?"
  },
  {
    "objectID": "Mini-Project #01.html#netflix-provides-over-200-weeks-of-service-history-for-all-but-one-country-in-our-data-set.-which-country-is-this-and-when-did-netflix-cease-operations-in-that-country",
    "href": "Mini-Project #01.html#netflix-provides-over-200-weeks-of-service-history-for-all-but-one-country-in-our-data-set.-which-country-is-this-and-when-did-netflix-cease-operations-in-that-country",
    "title": "Mini-Project #01",
    "section": "Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?",
    "text": "Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?"
  },
  {
    "objectID": "Mini-Project #01.html#what-is-the-total-viewership-of-the-tv-show-squid-game-note-that-there-are-three-seasons-total-and-we-are-looking-for-the-total-number-of-hours-watched-across-all-seasons.",
    "href": "Mini-Project #01.html#what-is-the-total-viewership-of-the-tv-show-squid-game-note-that-there-are-three-seasons-total-and-we-are-looking-for-the-total-number-of-hours-watched-across-all-seasons.",
    "title": "Mini-Project #01",
    "section": "What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.",
    "text": "What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons."
  },
  {
    "objectID": "Mini-Project #01.html#the-movie-red-notice-has-a-runtime-of-1-hour-and-58-minutes.-approximately-how-many-views-did-it-receive-in-2021-note-that-netflix-does-not-provide-the-weekly_views-values-that-far-back-in-the-past-but-you-can-compute-it-yourself-using-the-total-view-time-and-the-runtime.",
    "href": "Mini-Project #01.html#the-movie-red-notice-has-a-runtime-of-1-hour-and-58-minutes.-approximately-how-many-views-did-it-receive-in-2021-note-that-netflix-does-not-provide-the-weekly_views-values-that-far-back-in-the-past-but-you-can-compute-it-yourself-using-the-total-view-time-and-the-runtime.",
    "title": "Mini-Project #01",
    "section": "The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.",
    "text": "The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\nHint: The year() function from the lubridate package may be helpful."
  },
  {
    "objectID": "Mini-Project #01.html#how-many-films-reached-number-1-in-the-us-but-did-not-originally-debut-there-that-is-find-films-that-first-appeared-on-the-top-10-chart-at-e.g.-number-4-but-then-became-more-popular-and-eventually-hit-number-1-what-is-the-most-recent-film-to-pull-this-off",
    "href": "Mini-Project #01.html#how-many-films-reached-number-1-in-the-us-but-did-not-originally-debut-there-that-is-find-films-that-first-appeared-on-the-top-10-chart-at-e.g.-number-4-but-then-became-more-popular-and-eventually-hit-number-1-what-is-the-most-recent-film-to-pull-this-off",
    "title": "Mini-Project #01",
    "section": "How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?",
    "text": "How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\nHint: You will want to create a new variable to identify films that topped the charts at any point during their run."
  },
  {
    "objectID": "Mini-Project #01.html#which-tv-showseason-hit-the-top-10-in-the-most-countries-in-its-debut-week-in-how-many-countries-did-it-chart",
    "href": "Mini-Project #01.html#which-tv-showseason-hit-the-top-10-in-the-most-countries-in-its-debut-week-in-how-many-countries-did-it-chart",
    "title": "Mini-Project #01",
    "section": "Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?",
    "text": "Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?"
  },
  {
    "objectID": "Mini-Project #01.html#how-many-different-countries-does-netflix-operate-in",
    "href": "Mini-Project #01.html#how-many-different-countries-does-netflix-operate-in",
    "title": "Mini-Project #01",
    "section": "How many different countries does Netflix operate in?",
    "text": "How many different countries does Netflix operate in?\nThere are 94 that Netflix operates in."
  },
  {
    "objectID": "Mini-Project-01.html",
    "href": "Mini-Project-01.html",
    "title": "Mini-Project #01",
    "section": "",
    "text": "How many different countries does Netflix operate in?\nThere are 94 that Netflix operates in.\n\n\nWhich non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\nAll Quiet on the Western Front spent the most cumulative weeks in the global top 10 with 23 weeks.\n\n\nWhat is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\nPushpa 2: The Rule (Reloaded Version) is the film with the longest run time with 224 minutes.\n\n\nFor each of the four categories, what program has the most total hours of global viewership?\nThe titles with the most total hours of global viewership were, Troll(top English film), Don’t Look Up(top Non-English film), Squid Game(Non-English TV Show), Wednesday(English TV Show).\n\n\nWhich TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\nMoney Heist was the show with the longest run from Pakistan with 127 weeks in the Top 10.\n\n\nNetflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\nRussia is the one country with a service history of less than 200. They ceased operation in 2022-02-27.\n\n\nWhat is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\nThe total viewership of the show Squid Game was 5,048,300,000 hours throughout the three season run.\n\n\nThe movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\nThe approximate view count for Red Notice in 2021 was 3,362,203.\n\n\nHow many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\nThere were 75 films that did not originally debut at 1. The most recent film to pull this off was KPop Demon Hunters.\n\n\nWhich TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\nThe show that hit the top 10 in the most countries in its debut week was Emily in Paris: Season 2 in 94 countries."
  },
  {
    "objectID": "Mini-Project-01.html#how-many-different-countries-does-netflix-operate-in",
    "href": "Mini-Project-01.html#how-many-different-countries-does-netflix-operate-in",
    "title": "Mini-Project #01",
    "section": "How many different countries does Netflix operate in?",
    "text": "How many different countries does Netflix operate in?\nThere are 94 that Netflix operates in."
  },
  {
    "objectID": "Mini-Project-01.html#stranger-things-season-5-absolute-cinema",
    "href": "Mini-Project-01.html#stranger-things-season-5-absolute-cinema",
    "title": "Mini-Project #01",
    "section": "Stranger Things Season 5: Absolute Cinema!",
    "text": "Stranger Things Season 5: Absolute Cinema!\nSitting in Netflix’s top 10 for 366 weeks, Stranger Things Season 4 left viewers on their feet! As Season 4 sat in the top spot of the weekly ranking for 31 weeks we meet Eleven and the gang to take on everything that has lead them here. Accumulating 2,967,980,000 hours watched over the span of the show’s lifetime, the epic conclusion for the generational show that put Netflix show production on the map is bound to be the talk of the town. Do not miss out on being one of millions watching as Stranger Things wraps things up for good."
  },
  {
    "objectID": "Mini-Project-01.html#netflix-growth-in-india",
    "href": "Mini-Project-01.html#netflix-growth-in-india",
    "title": "Mini-Project #01",
    "section": "Netflix Growth in India",
    "text": "Netflix Growth in India\nAs the most populated country in the world, there is great opportunity for Netflix to grow greatly from growing business in India. As more media reaches India, there are more opportunities to benefit from the great spread of differing cultural arts and the consumption of local content. Many foreign shows and films are reaching into India’s top 10, ranging from WWE Smackdown to Squid Games. Though there is plenty of foreign media, there is also plenty of domestic media that remains in the top 10 such as, The Great Indian Kapil Show. There is much growth to be had as the expansion of foreign media looms greatly bringing the global top 10 into India’s perview."
  },
  {
    "objectID": "Mini-Project-01.html#netflix-built-on-the-binge",
    "href": "Mini-Project-01.html#netflix-built-on-the-binge",
    "title": "Mini-Project #01",
    "section": "Netflix: Built on the Binge",
    "text": "Netflix: Built on the Binge\nThe binge factor for streaming is one that cannot be ignored. As we explore the data, we find that there is a larger grasp that television has as it pertains to the streaming time. This makes sense when viewing from a pure content quantity but it definitely follows when we look at the duration that content sits in the top 10. If we look at what content has cumulatively been in the top 10 for more than 20 weeks, we find that it the category the content belongs to tends to be TV. Of the 59 piece of content that share this criteria, only 8 are film and 51 are TV shows. Netflix and streaming in general is overwhelmingly contingent on the ability to binge the content being presented and TV shows tend to be the primary guide to that as weekly stream hours are boosted by the needed to complete a show’s season over and over again."
  },
  {
    "objectID": "mp01.html#stranger-things-season-5-absolute-cinema",
    "href": "mp01.html#stranger-things-season-5-absolute-cinema",
    "title": "Mini-Project #01",
    "section": "Stranger Things Season 5: Absolute Cinema!",
    "text": "Stranger Things Season 5: Absolute Cinema!\nSitting in Netflix’s top 10 for 366 weeks, Stranger Things Season 4 left viewers on their feet! As Season 4 sat in the top spot of the weekly ranking for 31 weeks we meet Eleven and the gang to take on everything that has lead them here. Accumulating 2,967,980,000 hours watched over the span of the show’s lifetime, the epic conclusion for the generational show that put Netflix show production on the map is bound to be the talk of the town. Do not miss out on being one of millions watching as Stranger Things wraps things up for good."
  },
  {
    "objectID": "mp01.html#netflix-growth-in-india",
    "href": "mp01.html#netflix-growth-in-india",
    "title": "Mini-Project #01",
    "section": "Netflix Growth in India",
    "text": "Netflix Growth in India\nAs the most populated country in the world, there is great opportunity for Netflix to grow greatly from growing business in India. As more media reaches India, there are more opportunities to benefit from the great spread of differing cultural arts and the consumption of local content. Many foreign shows and films are reaching into India’s top 10, ranging from WWE Smackdown to Squid Games. Though there is plenty of foreign media, there is also plenty of domestic media that remains in the top 10 such as, The Great Indian Kapil Show. There is much growth to be had as the expansion of foreign media looms greatly bringing the global top 10 into India’s perview."
  },
  {
    "objectID": "mp01.html#netflix-built-on-the-binge",
    "href": "mp01.html#netflix-built-on-the-binge",
    "title": "Mini-Project #01",
    "section": "Netflix: Built on the Binge",
    "text": "Netflix: Built on the Binge\nThe binge factor for streaming is one that cannot be ignored. As we explore the data, we find that there is a larger grasp that television has as it pertains to the streaming time. This makes sense when viewing from a pure content quantity but it definitely follows when we look at the duration that content sits in the top 10. If we look at what content has cumulatively been in the top 10 for more than 20 weeks, we find that it the category the content belongs to tends to be TV. Of the 59 piece of content that share this criteria, only 8 are film and 51 are TV shows. Netflix and streaming in general is overwhelmingly contingent on the ability to binge the content being presented and TV shows tend to be the primary guide to that as weekly stream hours are boosted by the needed to complete a show’s season over and over again."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02",
    "section": "",
    "text": "if(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    library(dplyr)\n    library(tidyr)\n    library(readr)\n    \n    if(!file.exists(fname)){\n        \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        # These were looked up manually on bls.gov after finding \n        # they were presented as ranges. Since there are only three\n        # it was easier to manually handle than to special-case everything else\n        naics_missing &lt;- tibble::tribble(\n            ~Code, ~title, ~depth, \n            \"31\", \"Manufacturing\", 1,\n            \"32\", \"Manufacturing\", 1,\n            \"33\", \"Manufacturing\", 1,\n            \"44\", \"Retail\", 1, \n            \"45\", \"Retail\", 1,\n            \"48\", \"Transportation and Warehousing\", 1, \n            \"49\", \"Transportation and Warehousing\", 1\n        )\n        \n        naics_table &lt;- bind_rows(naics_table, naics_missing)\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n            drop_na() |&gt;\n            mutate(across(contains(\"code\"), as.integer))\n        \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\nlibrary(dplyr)\nPERMITS\n\n# A tibble: 5,658 × 3\n    CBSA new_housing_units_permitted  year\n   &lt;dbl&gt;                       &lt;dbl&gt; &lt;dbl&gt;\n 1 10180                         214  2009\n 2 10420                         741  2009\n 3 10500                         213  2009\n 4 10580                        1380  2009\n 5 10740                        1692  2009\n 6 10780                         396  2009\n 7 10900                        1648  2009\n 8 11020                         125  2009\n 9 11100                         642  2009\n10 11180                         122  2009\n# ℹ 5,648 more rows\n\nWAGES\n\n# A tibble: 4,442,181 × 6\n    YEAR FIPS  INDUSTRY EMPLOYMENT TOTAL_WAGES AVG_WAGE\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1  2009 C1018      101       8050   342280951   42519.\n 2  2009 C1018     1011       1769    86660038   48988.\n 3  2009 C1018     1012       3328   151822573   45620.\n 4  2009 C1018     1013       2952   103798340   35162.\n 5  2009 C1018      102      42334  1284997543   30354.\n 6  2009 C1018     1021      11993   368955371   30764.\n 7  2009 C1018     1022          0           0      NA \n 8  2009 C1018     1023       3575   138206437   38659.\n 9  2009 C1018     1024       4697   149886226   31911.\n10  2009 C1018     1025      11950   452254692   37846.\n# ℹ 4,442,171 more rows\n\nINDUSTRY_CODES\n\n# A tibble: 798 × 8\n   level1_title   level2_title level3_title level4_title level1_code level2_code\n   &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;\n 1 Agriculture, … Crop produc… Oilseed and… Soybean far…          11         111\n 2 Agriculture, … Crop produc… Oilseed and… Oilseed (ex…          11         111\n 3 Agriculture, … Crop produc… Oilseed and… Dry pea and…          11         111\n 4 Agriculture, … Crop produc… Oilseed and… Wheat farmi…          11         111\n 5 Agriculture, … Crop produc… Oilseed and… Corn farming          11         111\n 6 Agriculture, … Crop produc… Oilseed and… Rice farming          11         111\n 7 Agriculture, … Crop produc… Oilseed and… Other grain…          11         111\n 8 Agriculture, … Crop produc… Vegetable a… Vegetable a…          11         111\n 9 Agriculture, … Crop produc… Fruit and t… Orange grov…          11         111\n10 Agriculture, … Crop produc… Fruit and t… Citrus (exc…          11         111\n# ℹ 788 more rows\n# ℹ 2 more variables: level3_code &lt;dbl&gt;, level4_code &lt;dbl&gt;\n\nINCOME\n\n# A tibble: 7,279 × 4\n   GEOID NAME                                           household_income  year\n   &lt;dbl&gt; &lt;chr&gt;                                                     &lt;dbl&gt; &lt;dbl&gt;\n 1 10140 Aberdeen, WA Micro Area                                   36345  2009\n 2 10180 Abilene, TX Metro Area                                    42931  2009\n 3 10300 Adrian, MI Micro Area                                     45640  2009\n 4 10380 Aguadilla-Isabela-San Sebasti?n, PR Metro Area            13470  2009\n 5 10420 Akron, OH Metro Area                                      47482  2009\n 6 10500 Albany, GA Metro Area                                     36218  2009\n 7 10540 Albany-Lebanon, OR Micro Area                             47669  2009\n 8 10580 Albany-Schenectady-Troy, NY Metro Area                    57677  2009\n 9 10700 Albertville, AL Micro Area                                37284  2009\n10 10740 Albuquerque, NM Metro Area                                46824  2009\n# ℹ 7,269 more rows\n\nRENT\n\n# A tibble: 7,279 × 4\n   GEOID NAME                                           monthly_rent  year\n   &lt;dbl&gt; &lt;chr&gt;                                                 &lt;dbl&gt; &lt;dbl&gt;\n 1 10140 Aberdeen, WA Micro Area                                 650  2009\n 2 10180 Abilene, TX Metro Area                                  712  2009\n 3 10300 Adrian, MI Micro Area                                   645  2009\n 4 10380 Aguadilla-Isabela-San Sebasti?n, PR Metro Area          363  2009\n 5 10420 Akron, OH Metro Area                                    723  2009\n 6 10500 Albany, GA Metro Area                                   624  2009\n 7 10540 Albany-Lebanon, OR Micro Area                           761  2009\n 8 10580 Albany-Schenectady-Troy, NY Metro Area                  833  2009\n 9 10700 Albertville, AL Micro Area                              579  2009\n10 10740 Albuquerque, NM Metro Area                              726  2009\n# ℹ 7,269 more rows\n\nPOPULATION\n\n# A tibble: 7,279 × 4\n   GEOID NAME                                           population  year\n   &lt;dbl&gt; &lt;chr&gt;                                               &lt;dbl&gt; &lt;dbl&gt;\n 1 10140 Aberdeen, WA Micro Area                             71797  2009\n 2 10180 Abilene, TX Metro Area                             160266  2009\n 3 10300 Adrian, MI Micro Area                               99837  2009\n 4 10380 Aguadilla-Isabela-San Sebasti?n, PR Metro Area     342495  2009\n 5 10420 Akron, OH Metro Area                               699935  2009\n 6 10500 Albany, GA Metro Area                              164238  2009\n 7 10540 Albany-Lebanon, OR Micro Area                      116584  2009\n 8 10580 Albany-Schenectady-Troy, NY Metro Area             857592  2009\n 9 10700 Albertville, AL Micro Area                          90399  2009\n10 10740 Albuquerque, NM Metro Area                         856216  2009\n# ℹ 7,269 more rows\n\nHOUSEHOLDS\n\n# A tibble: 7,279 × 4\n   GEOID NAME                                           households  year\n   &lt;dbl&gt; &lt;chr&gt;                                               &lt;dbl&gt; &lt;dbl&gt;\n 1 10140 Aberdeen, WA Micro Area                             27759  2009\n 2 10180 Abilene, TX Metro Area                              58052  2009\n 3 10300 Adrian, MI Micro Area                               36835  2009\n 4 10380 Aguadilla-Isabela-San Sebasti?n, PR Metro Area      91805  2009\n 5 10420 Akron, OH Metro Area                               281769  2009\n 6 10500 Albany, GA Metro Area                               60101  2009\n 7 10540 Albany-Lebanon, OR Micro Area                       43953  2009\n 8 10580 Albany-Schenectady-Troy, NY Metro Area             336492  2009\n 9 10700 Albertville, AL Micro Area                          32651  2009\n10 10740 Albuquerque, NM Metro Area                         334647  2009\n# ℹ 7,269 more rows\n\nHOUSEHOLD_INCOMES &lt;- left_join(HOUSEHOLDS, INCOME, join_by(GEOID == GEOID, year == year, NAME == NAME)) |&gt; mutate(state = str_extract(NAME, \", (.{2})\", group=1))\n\nHOUSEHOLD_INCOMES\n\n# A tibble: 7,279 × 6\n   GEOID NAME                            households  year household_income state\n   &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;\n 1 10140 Aberdeen, WA Micro Area              27759  2009            36345 WA   \n 2 10180 Abilene, TX Metro Area               58052  2009            42931 TX   \n 3 10300 Adrian, MI Micro Area                36835  2009            45640 MI   \n 4 10380 Aguadilla-Isabela-San Sebasti?…      91805  2009            13470 PR   \n 5 10420 Akron, OH Metro Area                281769  2009            47482 OH   \n 6 10500 Albany, GA Metro Area                60101  2009            36218 GA   \n 7 10540 Albany-Lebanon, OR Micro Area        43953  2009            47669 OR   \n 8 10580 Albany-Schenectady-Troy, NY Me…     336492  2009            57677 NY   \n 9 10700 Albertville, AL Micro Area           32651  2009            37284 AL   \n10 10740 Albuquerque, NM Metro Area          334647  2009            46824 NM   \n# ℹ 7,269 more rows\n\nHOUSEHOLDS_INCOMES_POPULATION &lt;- left_join(HOUSEHOLD_INCOMES, POPULATION, join_by(GEOID == GEOID, year == year, NAME == NAME))\nHOUSEHOLDS_INCOMES_POPULATION \n\n# A tibble: 7,279 × 7\n   GEOID NAME                 households  year household_income state population\n   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 10140 Aberdeen, WA Micro …      27759  2009            36345 WA         71797\n 2 10180 Abilene, TX Metro A…      58052  2009            42931 TX        160266\n 3 10300 Adrian, MI Micro Ar…      36835  2009            45640 MI         99837\n 4 10380 Aguadilla-Isabela-S…      91805  2009            13470 PR        342495\n 5 10420 Akron, OH Metro Area     281769  2009            47482 OH        699935\n 6 10500 Albany, GA Metro Ar…      60101  2009            36218 GA        164238\n 7 10540 Albany-Lebanon, OR …      43953  2009            47669 OR        116584\n 8 10580 Albany-Schenectady-…     336492  2009            57677 NY        857592\n 9 10700 Albertville, AL Mic…      32651  2009            37284 AL         90399\n10 10740 Albuquerque, NM Met…     334647  2009            46824 NM        856216\n# ℹ 7,269 more rows\nlibrary(dplyr)\n\nCBSA1019 &lt;- PERMITS %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2019)\nmax_CBSA1019 &lt;- CBSA1019 %&gt;%\n  slice_max(order_by = new_housing_units_permitted, n = 1, with_ties = FALSE)\nmax_CBSA1019_name &lt;- max_CBSA1019 %&gt;%\n  pull(CBSA)\nmax_CBSA1019_name\n\n[1] 35620"
  },
  {
    "objectID": "mp02.html#proposed-sponsors",
    "href": "mp02.html#proposed-sponsors",
    "title": "Mini-Project #02",
    "section": "Proposed Sponsors",
    "text": "Proposed Sponsors\nA sponsor from Loredo, Texas and a cosponsor from Mrytle Beach, South Carolina would be a great step towards championing YIMBY as they are two of the major CBSAs that have shown proof of YIMBY benefits. They each experienced lowered rent burdens while also increasing household growth. Their success can be an indicator of further success and can champion the work we look to motivate the federal government to take action."
  },
  {
    "objectID": "mp02.html#how-we-can-gather-local-support",
    "href": "mp02.html#how-we-can-gather-local-support",
    "title": "Mini-Project #02",
    "section": "How We can Gather Local Support",
    "text": "How We can Gather Local Support\nAs a developmental project, it would be beneficial to out reach to coalitions that would benefit from increased affordable housing development. The primary groups that would benefit would be the working class that would benefit from lowered rent burdens, this would include most people in the lower middle class such as service workers or essential workers. The other group that would heavily benefit from the policy change would be construction and building companies. The increase of federally funded development projects keeps more workers employed and plays into the longevity of job security in the sector as well as a promise of expense easing from the lowering rent burden."
  },
  {
    "objectID": "mp02.html#important-metrics-to-note",
    "href": "mp02.html#important-metrics-to-note",
    "title": "Mini-Project #02",
    "section": "Important Metrics to Note",
    "text": "Important Metrics to Note\nThe metric that stands out the most and helps our case the most is our Rent Burden metric that shows the amount of household income that is spent on rent. This is a telling data metric to show the direction that our policy action or lack there of would take on the community. The improvement in this metric as it declines overtime would indicate greater affordability and success in our favor. Another important metric would be housing growth as we desire to create sustained growth that could fight the demand for housing and increase population growth, bolstering the ecoonomy of that community."
  },
  {
    "objectID": "mp02.html#why-fight",
    "href": "mp02.html#why-fight",
    "title": "Mini-Project #02",
    "section": "Why fight?",
    "text": "Why fight?\nThis bill would give local governments the ability and incentive to help the working class and the people who keep the economy afloat. This bill would support working class families and combat the continuously increasing cost of living that wages do not keep up with. YIMBY is policy choice that chooses not to price people out and filter the less fortunate. It looks to be the tide that raises all ships, giving those who need it a leg to stand on."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\n\nggplot() +\n  \n  # Layer 1: Council district polygons\n  geom_sf(data = DATA,\n          fill = NA,\n          color = \"black\",\n          linewidth = 0.3) +\n  \n  # Layer 2: Tree points\n  geom_sf(data = trees,\n          aes(),\n          color = \"darkgreen\",\n          alpha = 0.01,\n          size = 0.01) +\n  \n  coord_sf() +\n  labs(\n    title = \"NYC Trees Overlaid on City Council Districts\",\n    caption = \"Data: NYC Open Data\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\n\n\ntrees_with_districts &lt;- st_join(\n  trees,\n  DATA,\n  join = st_intersects\n)\n\n\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\n\n\ndistrict_count &lt;- trees_with_districts %&gt;%\n  st_drop_geometry() %&gt;%\n  count(CounDist) \n\nmax_tree_count &lt;- district_count %&gt;%\n  slice_max(order_by = n, n = 1, with_ties = FALSE) %&gt;%\n  pull(CounDist) \n\n\n\nWhich council district has the most trees?\nThe council district with the most trees is 51.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\n\ntrees_w_districts &lt;- st_join(\n  trees, DATA %&gt;% \n    select(CounDist, Shape_Area),  join = st_intersects\n)\ndistrict_count &lt;- trees_w_districts %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(CounDist) %&gt;%\n  summarise(\n    n = n(),\n    Shape_Area = first(Shape_Area)\n  )\ntree_density &lt;- district_count %&gt;%\n  mutate(tree_density = n / Shape_Area)\n\nhighest_density &lt;- tree_density %&gt;%\n  slice_max(order_by = n, n = 1, with_ties = FALSE) %&gt;%\n  pull(CounDist) \n\n\n\n\nWhich council district has the highest density of trees?\nThe council district with the highest density of trees is 51.\n\n\nCode\ndistrict_count &lt;- trees_with_districts %&gt;%\n  st_drop_geometry() %&gt;%\n  count(CounDist)\ndistrict_count_dead &lt;- trees_with_districts %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(tpcondition == \"Dead\" ) %&gt;%\n  count(CounDist)\n\ndistrict_count_dead &lt;- left_join(district_count, district_count_dead, \n            join_by(CounDist == CounDist)) %&gt;%\n  mutate(ratio_dead = n.y / n.x)\n\nmost_dead &lt;- district_count_dead %&gt;%\n  slice_max(order_by = ratio_dead, n = 1, with_ties = FALSE) %&gt;%\n  pull(CounDist)\n\nmost_dead\n\n\n[1] 32\n\n\n\n\nWhich district has highest fraction of dead trees out of all trees?\nThe district with the highest fraction of dead trees is 32.\n\n\nCode\nlibrary(dplyr)\n\n\ntrees_with_districts &lt;- trees_with_districts %&gt;%\n  mutate(Borough = \n           case_when(\n             CounDist %in% 1:10 ~ \"Manhattan\",\n        CounDist %in% 11:18 ~ \"Bronx\",\n        CounDist %in% 19:32 ~ \"Brooklyn\",\n        CounDist %in% 33:48 ~ \"Queens\",\n        CounDist %in% 49:51 ~ \"Staten Island\",\n        TRUE ~ \"Unknown\"\n           )\n  )\n\nspecies_in_manhattan &lt;- trees_with_districts %&gt;%\n  filter(Borough == \"Manhattan\") %&gt;%\n    st_drop_geometry() %&gt;%\n  count(genusspecies)\n\nmost_common_manhattan &lt;- species_in_manhattan %&gt;%\n  slice_max(order_by = n, n = 1, with_ties = FALSE) %&gt;%\n  pull(genusspecies)\n\n\n###What is the most common tree species in Manhattan?\nThe most common tree species in Manhattan is Gleditsia triacanthos var. inermis - Thornless honeylocust.\n\n\nCode\nlibrary(dplyr)\n\nnew_st_point &lt;- function(lat, lon, ...){\n    # st_sfc expects x, y which flips the normal lat (N/S) + lon (W/E) ordering\n    st_sfc(point = st_point(c(lon, lat))) |&gt;\n      st_set_crs(\"WGS84\")\n}\n\nbaruch_point &lt;- new_st_point(lat = 40.7404, lon = -73.9832)\n\ndist_baruch &lt;- trees_with_districts %&gt;%\n  filter(Borough == \"Manhattan\") %&gt;%\n  mutate(distance = st_distance(geometry, baruch_point))\n\nclosest_species &lt;- dist_baruch %&gt;%\n  slice_min(order_by = distance, n = 1, with_ties = FALSE) %&gt;%\n  pull(genusspecies)\n\nclosest_tree &lt;- dist_baruch %&gt;%\n  slice_min(order_by = distance, n = 1, with_ties = FALSE) \n\n\n###What is the species of the tree closest to Baruch’s campus? The species of the tree closest to Baruch Campus is Liquidambar styraciflua - sweetgum.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\nmanhattan_trees &lt;- trees_with_districts %&gt;%\n  filter(Borough == \"Manhattan\")%&gt;%\n      st_drop_geometry() %&gt;%\n  count(CounDist)\n\nmanhattan_dead_trees &lt;- trees_with_districts %&gt;%\n  filter(Borough == \"Manhattan\") %&gt;%\n  filter(tpcondition == \"Dead\") %&gt;%\n      st_drop_geometry() %&gt;%\n  count(CounDist)\n\nmanhattan_dead_trees &lt;- left_join(manhattan_trees, manhattan_dead_trees, \n            join_by(CounDist == CounDist)) %&gt;%\n  mutate(ratio_dead = n.y / n.x)\n\nmanhat_most_dead &lt;- manhattan_dead_trees %&gt;%\n  slice_max(order_by = ratio_dead, n = 1, with_ties = FALSE) %&gt;%\n  pull(CounDist)\n\ndata_dist_two &lt;- DATA %&gt;%\n    filter(CounDist == 2)\n\n\ndistr_two &lt;- trees_with_districts %&gt;%\n  filter(CounDist == 2)\n\ndistr_two_count &lt;- distr_two %&gt;%\n      st_drop_geometry() %&gt;%\n  count(CounDist)\n\ndistr_two_dead &lt;- trees_with_districts %&gt;%\n  filter(CounDist == 2) %&gt;%\n  filter(tpcondition == \"Dead\")\n\n\ndistr_two_dead_count &lt;- distr_two %&gt;%\n  filter(tpcondition == \"Dead\") %&gt;%\n      st_drop_geometry() %&gt;%\n  count(CounDist)\n\ndead_tree_plot &lt;- ggplot() +\n  \n  # Layer 1: Council district polygons\n  geom_sf(data = data_dist_two,\n          fill = NA,\n          color = \"black\",\n          linewidth = 0.3) +\n  \n  # Layer 2: Tree points\n  geom_sf(data = distr_two_dead,\n          aes(),\n          color = \"red\",\n          alpha = 1,\n          size = 1) +\n  \n  coord_sf() +\n  labs(\n    title = \"District 2's Dead Trees Overlaid\",\n    caption = \"Data: NYC Open Data\"\n  ) +\n  theme_minimal()\ndead_tree_plot\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\ndata_dist_two &lt;- DATA %&gt;%\n    filter(CounDist == 2)\n\ndistr_two_alive &lt;- trees_with_districts %&gt;%\n  filter(CounDist == 2) %&gt;%\n  filter(tpcondition != \"Dead\")\n\ndistr_two_dead &lt;- trees_with_districts %&gt;%\n  filter(CounDist == 2) %&gt;%\n  filter(tpcondition == \"Dead\")\n\ndist_two_plot &lt;- ggplot() +\n  \n  # Layer 1: Council district polygons\n  geom_sf(data = data_dist_two,\n          fill = NA,\n          color = \"black\",\n          linewidth = 0.3) +\n  \n  # Layer 2: Tree points\n  geom_sf(data = distr_two_alive,\n          aes(),\n          color = \"darkgreen\",\n          alpha = .5,\n          size = .5) +\n  \n  # Layer 3: Tree points\n  geom_sf(data = distr_two_dead,\n          aes(),\n          color = \"red\",\n          alpha = .5,\n          size = .5) +\n  \n  coord_sf() +\n  labs(\n    title = \"District 2's Trees, Dead or Alive Overlaid\",\n    caption = \"Data: NYC Open Data\"\n  ) +\n  theme_minimal()\n\ndata_dist_three &lt;- DATA %&gt;%\n    filter(CounDist == 3)\n\ndistr_three_alive &lt;- trees_with_districts %&gt;%\n  filter(CounDist == 3) %&gt;%\n  filter(tpcondition != \"Dead\")\n\ndistr_three_dead &lt;- trees_with_districts %&gt;%\n  filter(CounDist == 3) %&gt;%\n  filter(tpcondition == \"Dead\")\n\ndist_three_plot &lt;- ggplot() +\n  \n  # Layer 1: Council district polygons\n  geom_sf(data = data_dist_three,\n          fill = NA,\n          color = \"black\",\n          linewidth = 0.3) +\n  \n  # Layer 2: Tree points\n  geom_sf(data = distr_three_alive,\n          aes(),\n          color = \"darkgreen\",\n          alpha = .5,\n          size = .5) +\n  \n  # Layer 3: Tree points\n  geom_sf(data = distr_three_dead,\n          aes(),\n          color = \"red\",\n          alpha = .5,\n          size = .5) +\n  \n  coord_sf() +\n  labs(\n    title = \"District 3's Trees, Dead or Alive Overlaid\",\n    caption = \"Data: NYC Open Data\"\n  ) +\n  theme_minimal()\n\n\n\n\nGovernment Tree Project Proposal\nWe are proposing to begin a tree project within district 2. We are seeking to being the replacement of the dead trees within Manhattan, beginning with district 2. District 2 was chose as it was the Manhattan district that had the highest ratio of dead trees. The start of their removal for replacement for new young trees would be of great benefit to the ecosystems and environments of the surrounding area. Seeing as there are 2, 11563 trees in district 2 but there are 2, 1576 dead trees, it would be in our best interest to begin on the project as soon as possible.\n\n\nCode\ndead_tree_plot\n\n\n\n\n\n\n\n\n\nThis project makes sense for this district as it has the highest ratio of dead trees of all districts within Manhattan. It would be a good place to start compared to the other 9.It may not have the most dead trees to tackle for the project but with the higher density, the effects of replacement may show quicker due to the difference in percentage of trees being replaced.\n\n\nCode\ndist_two_plot \n\n\n\n\n\n\n\n\n\nCode\ndist_three_plot\n\n\n\n\n\n\n\n\n\nLooking at both district 2 and 3, we can see that there are less problem areas across the whole district. When cleaning theses areas, there should be a noticable change in the environment and ecosystem with these new healthy trees being put in their place."
  },
  {
    "objectID": "mp02.html#context",
    "href": "mp02.html#context",
    "title": "Mini-Project #02",
    "section": "Context",
    "text": "Context\nAmerica has had a long standing housing crisis that has driven rents to record highs and priced out millions of families out of their communities. While there are some cities that work to increase housing supplies and stabilize rents for families, there are many that continue to halt and slow development of affordable housing. Through our analysis, there are regions that embrace “Yes In My Backyard” (YIMBY) policies that both increase population growth as well as lower rent burden over the time of development, while restrictive “Not In My Backyard” (NIMBY) policies show signs of higher costs and population stagnation."
  }
]